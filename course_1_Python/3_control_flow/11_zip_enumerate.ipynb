{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Zip Model Metrics\n",
    "\n",
    "Use `zip` to write a for loop that creates a string specifying the model name and its corresponding metrics (accuracy, precision, recall) and appends it to the list `model_metrics`.\n",
    "\n",
    "Each string should be formatted as `model: accuracy, precision, recall`. For example, the string for the first model should be `Model1: 0.95, 0.94, 0.93`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model1: 0.95, 0.94, 0.93', 'Model2: 0.89, 0.88, 0.87', 'Model3: 0.92, 0.91, 0.91', 'Model4: 0.87, 0.86, 0.85', 'Model5: 0.93, 0.92, 0.91']\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"Model1\", \"Model2\", \"Model3\", \"Model4\", \"Model5\"]\n",
    "accuracy = [0.95, 0.89, 0.92, 0.87, 0.93]\n",
    "precision = [0.94, 0.88, 0.91, 0.86, 0.92]\n",
    "recall = [0.93, 0.87, 0.91, 0.85, 0.91]\n",
    "model_metrics = []\n",
    "\n",
    "# write your for loop here\n",
    "for name,acc,pre,rec in zip(model_names,accuracy,precision,recall):\n",
    "    model_metrics.append(f\"{name}: {acc}, {pre}, {rec}\")\n",
    "\n",
    "print(model_metrics)\n",
    "\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = [\"Model1: 0.95, 0.94, 0.93\", \"Model2: 0.89, 0.88, 0.87\", \"Model3: 0.92, 0.91, 0.91\", \"Model4: 0.87, 0.86, 0.85\", \"Model5: 0.93, 0.92, 0.91\"]\n",
    "if model_metrics == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Zip Metrics to Dictionary\n",
    "\n",
    "Use `zip` to create a dictionary `model_performance` that uses `model_names` as keys and `accuracies` as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model1': 0.95, 'Model2': 0.89, 'Model3': 0.92, 'Model4': 0.87, 'Model5': 0.93}\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"Model1\", \"Model2\", \"Model3\", \"Model4\", \"Model5\"]\n",
    "accuracies = [0.95, 0.89, 0.92, 0.87, 0.93]\n",
    "\n",
    "model_performance = {}\n",
    "\n",
    "for name,acc in zip(model_names,accuracies):\n",
    "    model_performance[name] = acc\n",
    "\n",
    "print(model_performance)\n",
    "\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = {\"Model1\": 0.95, \"Model2\": 0.89, \"Model3\": 0.92, \"Model4\": 0.87, \"Model5\": 0.93}\n",
    "if model_performance == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Unzip Model Metrics\n",
    "\n",
    "Unzip the `model_performance` tuple into two `model_names` and `accuracies` tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Model1', 'Model2', 'Model3', 'Model4', 'Model5') (0.95, 0.89, 0.92, 0.87, 0.93)\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "model_performance = ((\"Model1\", 0.95), (\"Model2\", 0.89), (\"Model3\", 0.92), (\"Model4\", 0.87), (\"Model5\", 0.93))\n",
    "\n",
    "# define model_names and accuracies here\n",
    "model_names,accuracies = zip(*model_performance)\n",
    "\n",
    "print(model_names,accuracies)\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer_names = (\"Model1\", \"Model2\", \"Model3\", \"Model4\", \"Model5\")\n",
    "correct_answer_accuracies = (0.95, 0.89, 0.92, 0.87, 0.93)\n",
    "if model_names == correct_answer_names and accuracies == correct_answer_accuracies:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Transpose Metrics Matrix\n",
    "\n",
    "Use `zip` to transpose `metrics_data` from a 4-by-3 matrix to a 3-by-4 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.95, 0.89, 0.92, 0.87), (0.94, 0.88, 0.91, 0.86), (0.93, 0.87, 0.9, 0.85)]\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "metrics_data = ((0.95, 0.94, 0.93), (0.89, 0.88, 0.87), (0.92, 0.91, 0.90), (0.87, 0.86, 0.85))\n",
    "\n",
    "# Transpose the matrix\n",
    "metrics_data_transpose = list(zip(*metrics_data))\n",
    "print(metrics_data_transpose)\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = [(0.95, 0.89, 0.92, 0.87), (0.94, 0.88, 0.91, 0.86), (0.93, 0.87, 0.90, 0.85)]\n",
    "if metrics_data_transpose == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Enumerate Model Performances\n",
    "\n",
    "Use `enumerate` to modify the `model_descriptions` list so that each element contains the model name followed by its corresponding accuracy. For example, the first element of `model_descriptions` should change from `\"Model1 Description\"` to `\"Model1 Description 0.95\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Model1 Description 0.95', 'Model2 Description 0.89', 'Model3 Description 0.92', 'Model4 Description 0.87', 'Model5 Description 0.93']\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "model_descriptions = [\"Model1 Description\", \"Model2 Description\", \"Model3 Description\", \"Model4 Description\", \"Model5 Description\"]\n",
    "accuracies = [0.95, 0.89, 0.92, 0.87, 0.93]\n",
    "\n",
    "# write your for loop here\n",
    "for i,model in enumerate(model_descriptions):\n",
    "    model_descriptions[i] = f\"{model} {accuracies[i]}\"\n",
    "print(model_descriptions)\n",
    "\n",
    "### Notebook grading\n",
    "correct_answer = [\"Model1 Description 0.95\", \"Model2 Description 0.89\", \"Model3 Description 0.92\", \"Model4 Description 0.87\", \"Model5 Description 0.93\"]\n",
    "if model_descriptions == correct_answer:\n",
    "    print(\"Good job!\")\n",
    "else:\n",
    "    print(\"Oops! It doesn't look like the expected answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "grader_mode": "",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "showGradeBtn": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
